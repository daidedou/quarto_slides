---
title: "Denoising diffusion using Stochastic Differential Equations"
author: "LIX Seminar"
format:
  revealjs
css: styles.css
---



## Reminders: score modeling

- Bypasses the normalizing constant problem
- Allows for sampling using Langevin algorithm
- We can learn a score function $s_\theta(x)$

## Evolution of the score modeling approach {auto-animate="true"}

<!-- ::: {data-timeline="welcome.json"}
::: -->

::: columns


::: {.column width="50%"}
First proposal of score matching

- <span style="color:red;">The quantity $Trace\left(\nabla_x^2 \log p_\theta(x)\right)$ is difficult to compute</span>
- <span style="color:red;">The score is untractable in low density areas</span>
:::

::: {.column width="50%" .r-fit-text}
Given $\{x_1, x_2, ..., x_T\} \sim p_\text{data}(x)$
Objective: Minimize the quantity
$$ E_{p(x)}\left[\frac{1}{2}|| \log p_{\theta}(x)||² + Trace\left(\nabla_x^2 \log p_\theta(x)\right)\right]$$

![&nbsp;](images/score_density.png)
:::
:::

## Evolution of the score modeling approach {auto-animate="true"}

<!-- ::: {data-timeline="welcome.json"}
::: -->

::: columns


::: {.column width="50%"}
First proposal of score matching

Learning the score of a noisy distribution

<span style="color:rgb(219, 79, 79); font-size: smaller;">No score of noise-free distribution</span>
:::

::: {.column width="50%"}
![&nbsp;](images/noisy_distribution.png)
:::
:::

## Evolution of the score modeling approach {auto-animate="true" .smaller}

<!-- ::: {data-timeline="welcome.json"}
::: -->

::: columns


::: {.column width="50%"}
First proposal of score matching

Learning the score of a noisy distribution

Denoising diffusion models, annealed Langevin dynamics

Gradually decrease noise in the distribution

Multiple variants of the DDPM algorithm we know

:::

::: {.column width="50%"}
![Annealed Langevin sampling](images/multi_scale.jpg)
:::
:::

## Evolution of the score modeling approach {auto-animate="true"}

<!-- ::: {data-timeline="welcome.json"}
::: -->

::: columns


::: {.column width="50%"}
- First proposal of score matching
- Learning the score of a noisy distribution
- Denoising diffusion models, annealed Langevin dynamics
- DDPM beats GAN!

:::

::: {.column width="50%"}
![](images/kanagawa.jpg)
:::
:::

## What is DDPM?
- Forward process
- Backward process 
- A denoiser $\epsilon_\theta(., t)$ 
- A simple training objective $L_\text{simple}=\left|\left|\epsilon-\epsilon_\theta\left(\underbrace{\sqrt{\bar{\alpha_t}}x_0+\sqrt{1-\bar{\alpha_t}}\epsilon}_{\text{Forward estimate of } x_t \text{ given } x_0},t\right)\right|\right|^2$
- This objective is equivalent to the score matching objective


## Algorithm


## Questions/Problems

- Can we unify DDPM and other approaches in a common framework?
- Usually, $T$ needs to be fixed before training 
- Can we fasten the sampling, ideally without needed re-training? 
- Can we model the data in a deterministic way using score modeling?

. . . 

Proposed solution: Score modeling using Stochastic Differential Equations!

## Ordinary Differential Equations (ODE)
::: columns


::: {.column width="20%".r-fit-text}
Equations of functions, of the form $\frac{dx}{dt} = f(x, t)$ (order 1).


- Unique solution for any initial condition $x(t_0)$
- Geometric interpretation using vector fields 

:::

::: {.column width="80%"}


<iframe src="https://www.geogebra.org/classic/v88mtcwe?embed" width="800" height="600" allowfullscreen style="border: 1px solid #e4e4e4;border-radius: 4px;" frameborder="0"></iframe>

:::
:::

## Stochastic diffential equations (SDE)

Equation of time-dependent stochastic processes, noted $X_t$.

. . . 

They are of the form $dx = \underbrace{f(x, t)dt}_{\text{"drift" term}} + \underbrace{g(t) dW_t}_{\text{"diffusion" term}}$,

where $W_t$ is a "standard Wiener process" or Brownian motion.

::: columns
They are used in 

::: {.column width="20%".r-fit-text}
- Finance
- Biological analysis
- Shape analysis

:::

::: {.column width="80%"}


<iframe src="https://www.geogebra.org/classic/v88mtcwe?embed" width="800" height="600" allowfullscreen style="border: 1px solid #e4e4e4;border-radius: 4px;" frameborder="0"></iframe>

:::
:::

## Differences between SDE and ODE

- Given an initial condition, an SDE has now multiple possible realizations!

- The initial condition is now always $x_0$, the time only goes Forward

- Multiple solvers exists to simulate the output of an SDE

## Brownian motion {.scrollable}

A stochastic process $W_t$ is a Wiener process, or Brownian motion, if:

* $W_0 = 0$
* $W_t$ is "almost surely" continuous
* $W_t$ has independent increments 
* $W_t - W_s \sim \mathcal{N}(0, t-s), \text{ for any } 0 \leq s \leq t$

```{python}
#from IPython.display import HTML
import matplotlib.pyplot as plt
import os 
from matplotlib.animation import FuncAnimation, FFMpegWriter
import numpy as np

video_file = 'animation.mp4'
if not os.path.exists(video_file):
    fig = plt.figure(figsize=(25, 10))
    ax = plt.axes(xlim=(0, 1))
    line, = ax.step([], [], where='mid', color='#0492C2')

    # formatting options
    ax.set_xticks(np.linspace(0,1,11))
    ax.set_xlabel('Time', fontsize=30)
    ax.set_ylabel('Value', fontsize=30)
    ax.tick_params(labelsize=22)
    ax.grid(True, which='major', linestyle='--', color='black', alpha=0.6)
    ax.set_ylim((-2, 2))


def brownian_motion(N, N_tot, T, h):
    """
    Simulates a Brownian motion
    :param int N : the number of discrete steps
    :param int T: the number of continuous time steps
    :param float h: the variance of the increments
    """
    dt = 1. * T/N_tot  # the normalizing constant
    random_increments = np.random.normal(0.0, 1.0 * h, N)*np.sqrt(dt)  # the epsilon values
    brownian_motion = np.cumsum(random_increments)  # calculate the brownian motion
    brownian_motion = np.insert(brownian_motion, 0, 0.0) # insert the initial condition

    return brownian_motion, random_increments

N = 5000 # the number of discrete steps
T = 1 # the number of continuous time steps
h = 1 # the variance of the increments
dt = 1.0 * T/N  # total number of time steps
n_frames = 1000

# initialization function 
def init():
    line.set_data([], [])
    return line,

# animation function 
def animate(i):
    np.random.seed(42)
    N_c = int(N/n_frames)
    y, epsilon = brownian_motion((i + 1) * N_c, N, 1 ,1)
    tr = np.linspace(0.0, dt * (i + 1) * N_c, (i+1)*N_c+1)
    ax.set_title('Brownian Motion, time {0}'.format((i + 1) * 10), fontsize=40)
    

    line.set_data(list(tr), list(y))
    return line,

# call the animator	 
video_file = 'animation.mp4'
if not os.path.exists(video_file):
    anim = FuncAnimation(fig, animate, init_func=init, frames=n_frames, interval=100, blit=True)
    FFwriter = FFMpegWriter(fps=10)
    anim.save('animation.mp4', writer = FFwriter)
plt.close()
```
<!-- {{< video animation.mp4 >}} -->
![](animation.mp4)

We can repeat over 100 realisations. This will give this kind of image:
```{python}
#from IPython.display import HTML
import matplotlib.pyplot as plt
import os 
from matplotlib.animation import FuncAnimation, FFMpegWriter
import numpy as np

fig = plt.figure(figsize=(25, 10))
ax = plt.axes(xlim=(0, 1))

# formatting options
ax.set_xticks(np.linspace(0,1,11))
ax.set_xlabel('Time', fontsize=30)
ax.set_ylabel('Value', fontsize=30)
ax.tick_params(labelsize=22)
ax.grid(True, which='major', linestyle='--', color='black', alpha=0.6)
ax.set_ylim((-2, 2))


def brownian_motion(N, N_tot, T, h):
    """
    Simulates a Brownian motion
    :param int N : the number of discrete steps
    :param int T: the number of continuous time steps
    :param float h: the variance of the increments
    """
    dt = 1. * T/N_tot  # the normalizing constant
    random_increments = np.random.normal(0.0, 1.0 * h, N)*np.sqrt(dt)  # the epsilon values
    brownian_motion = np.cumsum(random_increments)  # calculate the brownian motion
    brownian_motion = np.insert(brownian_motion, 0, 0.0) # insert the initial condition

    return brownian_motion, random_increments

N = 5000 # the number of discrete steps
T = 1 # the number of continuous time steps
h = 1 # the variance of the increments
dt = 1.0 * T/N  # total number of time steps
n_lines = 100
for i in range(n_lines):
    y, epsilon = brownian_motion(N, N, 1 ,1)
    tr = np.linspace(0.0, 1, N+1)
    ax.plot(tr, y)

plt.show()
```

## Stochastic Differential Equation example

## Properties of the SDE 

Let the SDE be:

$$
dx = f(x, t)dt + g(t) d\bar{w}
$$

- Assuming some conditions on $f(x, t)$ and $g(t)$, the density $p_t(x)$ at any time step is uniquely determined

- Suppose we know, the score $\nabla_x \log  p_t(x)$ for all $t$, then we can reverse the diffusion SDE using the following reverse-SDE:

$$
dx = \left[f(x,t) - g(t)² \nabla_x \log p_t(x)\right] dt + g(t) d\bar{w},
$$

where the time is flowing backwards, and $\bar{w}$ is a backward Wiener process.

## "Variance Exploding" SDE {.scrollable}

Denoising Score matching of 2019. We perturb the data by adding noise $\tilde{x}_i \sim \mathcal{N}(x, \sigma_i² I). The forward process is such that 

$$
x_i = x_{i-1} + \sqrt{\sigma_i² - \sigma_{i-1}² z_{i-1}}, z_{i-1} \sim \mathcal{N}(0, I)
$$

Seen as the discretization of a continuous process, it becomes: 

$$
\begin{align}
x(t + \Delta t) & = x(t) + \sqrt{\sigma²(t+\Delta t) - \sigma²(t)} z(t)  \\
& \simeq x(t) + \sqrt{\frac{d\left[\sigma²(t)\right]}{dt} \Delta t} z(t)
\end{align}
$$

The SDE formulates as 
$$
dx = \sqrt{\frac{d\left[\sigma²(t)\right]}{dt}} dw
$$


## "Variance preserving" SDE {.scrollable}

This formulation is equivalent to the DDPM paper. The forward process of DDPM is given by:
$$
x_i = \sqrt{1-\beta_i} x_{i-1} + \sqrt{\beta_i} z_i = \sqrt{1-\frac{\bar{\beta}_i}{N}} x_{i-1} + \sqrt{\frac{\bar{\beta}_i}{N}} z_i 
$$

Where $\bar{\beta_i} = N \beta_i$. 

Seen as the discretization of a continuous process, it becomes: 

::: {.r-fit-text}
$$
\begin{align}
x(t + \Delta t) & = \sqrt{1-\beta(t+\Delta t) \Delta t}x(t) + \sqrt{\beta(t+\Delta t) \Delta t} z(t) \\
& \simeq x(t) - \frac{1}{2} \beta(t+\Delta t) \Delta t x(t) + \sqrt{\beta(t+\Delta t) \Delta t} z(t) \text{ (Taylor expansion) } \\
& \simeq x(t) - \frac{1}{2} \beta(t) \Delta t x(t) + \sqrt{\beta(t) \Delta t} z(t)
\end{align}
$$
::: 
The SDE formulates as 
$$
dx = - \frac{1}{2} \beta(t) x dt + \sqrt{\beta(t)}dw
$$

## Comparisons of behavior of both SDEs 

With original data distribution -> two diracs

![](images/variance_sdes.png)

The variance exploding has more curvature when approaching data $\to$ DDPM obtains better samples with fewer steps.

::: {.smaller}
Note: other samplers, such as DDIM or EDM can show even better curvature
:::

## Results 

## Probability flow ODE 

Another remarkable result: Given the following SDE: 

$$
dx = f(x, t)dt + g(t) d\bar{w},
$$

the following ODE :

$$
dx = \left[f(x, t) - g(t) \nabla_x \log p(x)\right] dt,
$$

share the same marginals $p_t(x)$ if $p_O$ or $p_T$ is given.

## Advantages of the ODE {.scrollable}

Uniqueness: a data sample $x(O)$ has a unique "latent" corresponding point $x(T)$.$

- First consequence: "latent" manipulation of images

![](images/interpolation.png)

- Second consequence: the model doesn't matter

![](images/latent_unique.png)

## Advantages of the ODE 

We can use well established solvers. It reduces the number of evaluation steps

![Samples of images, with different number of network evaluations](images/number_evaluations.png)

Compared to stochastic sampling, it reduces the number of needed evaluations from $\mathcal{O}(d)$ to $\mathcal{O}(\sqrt{d})$, where $d$ is the dimension of the data.